### YamlMime:FAQ
metadata:
  title: Azure Data Explorer ingestion FAQ 
  description: "Get answers to common questions about Azure Data Explorer ingestion."
  ms.date: 10/21/2021
title: Common questions about Azure Data Explorer ingestion
summary: This article answers commonly asked questions about Azure Data Explorer ingestion.
sections: 
  - name: Batching and data latencies
    questions:
      - question: How does batching affect my data?
        answer: |
          The batching manager buffers and batches ingress data based on the ingestion settings in the [ingestion batching policy](batchingpolicy.md). The ingestion batching policy sets batch limits according to three limiting factors, whichever is first reached: time elapsed since batch creation, accumulated number of items (blobs), or total batch size. The default batching settings are 5 minutes / 1 GB / 1000 blobs, meaning there will be at least a 5 minute delay when queueing a sample data for ingestion.
      - question: Should I use batching or streaming ingestion?
        answer: |
          Batching ingestion is optimized for high ingestion throughput. Streaming ingestion is ongoing data ingestion from a streaming source. Learn more about [batching versus streaming ingestion](../../ingest-data-overview.md#batching-vs-streaming-ingestion).
      - question: Do I need to change the batching policy?
        answer: |
          If default settings for the [ingestion batching policy](batchingpolicy.md) do not suit your needs, you can try lowering the batching policy `time`. See also [ingestion best practices - optimizing for throughput](../api/netfx/kusto-ingest-best-practices.md#optimizing-for-throughput). You should also update settings when you scale up ingestion. When you change batching policy settings, it can take up to 5 minutes to take effect. 
      - question: What causes batching latency?
        answer: |
          Ingestion latency can result from the [ingestion batching policy](batchingpolicy.md) settings, or a data backlog buildup. To address this, adjust the [batching policy settings](batching-policy.md).
          Latencies that are part of the ingestion process can be [monitored](../../monitor-batching-ingestion.md).      
      - question: Where can I view batching latency metrics?
        answer: |
          To view ingestion latency metrics, see [monitoring ingestion latency](../../monitor-batching-ingestion.md#view-the-ingestion-latency). The metrics `Stage Latency` and `Discovery Latency` show latencies in the ingestion process, and reveal if there are any long latencies.
      - question: How can I shorten batching latencies? 
        answer: |
          You can [change settings in the batching policy](batchingpolicy.md#batching-latencies) to address issues that cause latencies such as data backlogs, inefficient batching, batching very large amounts of uncompressed data, or ingesting very small amounts of data. 
      - question: How is batching data size calculated? 
        answer: |
          The batching policy data size is set for uncompressed data. When ingesting compressed data, the [uncompressed data size is calculated](batchingpolicy.md#batch-data-size) from ingestion batching parameters, zip files metadata, or factor over the compressed file size.
  - name: Ingestion monitoring, metrics and errors
    questions: 
      - question: Where can I view ingestion metrics?
        answer: |
          To view ingestion metrics, see [monitoring ingestion metrics](../../using-metrics.md).
      - question: How can I monitor for ingestion failures?
        answer: |
          Ingestion failures should be monitored [using metrics](../../using-metrics.md#ingestion-metrics), and by [setting up and using ingestion diagnostic logs](../../using-diagnostic-logs.md), for detailed table-level monitoring, viewing detailed ingestion error codes, and so on. 
          For streaming metrics, see streaming [metrics](../../using-metrics.md#streaming-ingest-metrics). For batching metrics, see           [using batching metrics](../../monitor-batching-ingestion.md).
      - question: Where can I view insights about ingestion?
        answer: |
          You can use the monitoring view in the portal's [Azure Monitor Insights](/azure/azure-monitor/insights/data-explorer). 
          This view is based on [metrics](../../using-metrics.md) and [diagnostic logs](../../using-diagnostic-logs.md) that 
          can be streamed to a [Query Log Analytics workspace.](media/query-monitor-data/query-la.png).  
      - question: How do I see ingestion errors?
        answer: |
          The [`.show ingestion failures`](ingestionfailures.md) command return ingestion failures that occur in the Kusto Data Engine. 
          Ingestion failures that occur during other parts of the ingestion flow, such as before data ingestion control commands 
          are sent to the Kusto Data Engine service, will not appear in the result set of this command. 
          Failures across the entire ingestion process appear in the metrics and diagnostic logs. 
      - question: What happens if you try to ingest too high a load?
          You might receive an error for `max retry exceeded`, which can happen when there is too high an ingestion load.  
          Too solve this issue, use a premium storage account or divide the ingested data over several storage accounts.
  - name: SDKs and connectors
    questions:     
      - question: How can I improve ingestion with SDKs?
        answer: |
          When ingesting via SDK, you can use the ingestion [batching policy settings to improve performance](../../net-sdk-ingest-data.md). 
          Try incrementally decreasing the size of data ingested in the table or database batching policy down towards 250 MB and check if there is an improvement.
      - question: How can I tune Kusto Kafka Sink for better ingestion performance?
        answer: |
          [Kafka Sink](https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md) users should [tune the connector](../../ingest-data-kafka.md) to work together with the [ingestion batching policy](batchingpolicy.md) by tuning batching time, size, and item number. 