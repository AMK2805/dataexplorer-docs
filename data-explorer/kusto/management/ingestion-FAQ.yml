### YamlMime:FAQ
metadata:
  title: Azure Data Explorer Ingestion FAQ 
  description: "Get answers to common questions about Azure Data Explorer ingestion."
  ms.date: 10/21/2021
title: Common questions about Azure Data Explorer ingestion
summary: This article answers commonly asked questions about Azure Data Explorer ingestion.
sections:
  - name: Batching and data latencies
    questions:
      - question: How does batching effect my data?
        answer: |
          A central component of the ingestion service is the batching manager, that buffers and batches ingress data based on the ingestion settings in the [ingestion batching policy](batchingpolicy.md). The ingestion batching policy sets batch limits according to three limiting factors, whichever is first reached: time elapsed since batch creation, accumulated number of items (blobs), or total batch size. The default batching settings are 5 minutes / 1 GB / 1000 blobs, meaning there will be at least a 5 minute delay when queueing sample data for ingestion.
      - question: Do I need to change the batching policy?
        answer: |
          You should define an [ingestion batching policy](batchingpolicy.md) before you start experimenting with ingestion, and adjust it accordingly. You should also update settings when you scale up ingestion. When you change batching policy settings, it can take up to 5 minutes to take effect. 
      - question: What causes batching latency?
        answer: |
          Ingestion latency can result from the [ingestion batching policy](batchingpolicy.md) settings, or a data backlog buildup. To address this, adjust the [batching policy settings](batching-policy.md).
          Latencies that are part of the ingestion process can be [monitored](../../monitor-batching-ingestion.md).      
      - question: Where can I view batching latency metrics?
        answer: |
          To view ingestion latency metrics, see [monitoring ingestion latency](../../monitor-batching-ingestion.md#view-the-ingestion-latency).
      - question: How can I shorten batching latencies?
        answer: |
          Latencies can result from: 
          - Data latency matching the time-based ingestion policy, so not enough data is fed to the table to pass the data-size or item-count limit and trigger the batch to be ingested. Try reducing the time.
          - Inefficient batching - if you ingest a large number of very small files, it could slow down ingestion and reduce performance. Try increasing the size of the source files. If you use Kusto Kafka Sink, configure it to send data to Kusto in ~100KB chunks or higher. Additionally, in the case of many small files, try increasing the number of files in each batch (up to 2000) by altering the database or table ingestion policy.  
          - Batching a large amount of uncompressed data can degrade performance - Azure Data Explorer is optimized to ingest 1GB of uncompressed data in each batch. 
          - Ingestion jobs with a very large uncompressed data size are common when ingesting Parquet files. Incrementally decrease the size of data ingested in the table or database batching policy towards 250MB and check for improvement.
          - Ingestion backlog can occur if the cluster is under-scaled for the amount of data it takes in. Consider accepting any Azure advisor suggestions to scale aside or scale up your cluster. Alternatively, manually scale your cluster to see if the backlog is closed.	If these options do not work, contact Azure Data Explorer support for assistance.
      - question: How is the batching data size computed?
        answer: |
          The batching policy data size is set for uncompressed data. When ingesting compressed data, the uncompressed data size if deduced as follows in descending order of accuracy:
          -	If the uncompressed size is provided in the ingestion source options, that value is used.
          -	When ingesting local files using SDKs, Azure Data Explorer may inspect zip archives and gzip streams to assess their raw size.
          -	Lastly, if previous options do not provide a data size, a factor is applied to the compressed data size to estimate the uncompressed data size.
      - question: How can I improve ingestion with SDK?
        answer: |
          When ingesting via SDK, you can use the â€˜Flush Immediately' ingestion property to skip the batching, but this is not recommended for large scale ingestion, as it can lead to poor performance. 
          If you send compressed data Azure Data Explorer SDK, consider passing the Raw Data Size in the ingestion source options parameter to avoid a wrong estimation. 
          Try incrementally decreasing the size of data ingested in the table or database batching policy down towards 250MB and check if there is an improvement.
      - question: How can I tune Kusto Kafka Sink for better ingestion performance?
        answer: |
          - Kusto Kafka Sink users should tune their connector to work together with the ingestion batching policy. 
          - There will be two batching policies running, and their batching size should be up to 1G. Ensure that the combined batching times suits your ingestion needs.
          - Try tuning the Kafka Sink size limit `flush.size.bytes` starting from 1MB, moving up or down in increments of 10 or 100.
          - For the batching policy, set the time limit to the highest number of seconds you can accept, and the number of items to at least 1000. Set batching size at 1G and increase by 100MB increments if needed.
          - Try increasing the Kafka Sink batching time if not enough data accumulates in each batch. 
          - You can scale by adding instances and partitions. Increase `tasks.max` to the number of partitions. You will want to increase partitions as long as you have enough data so that you produce blobs the size of the `flush.size.bytes` settings. If blobs are smaller than this size, batches are processed according to the batching time limit, meaning that the partition isn't receiving enough throughput. Be aware, however, that a large number of partitions means more processing overhead.
